{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c564a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2f0d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f4b140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03cff885",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30d20c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Prime Minister of India - Wikipedia',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Prime_Minister_of_India',\n",
       "  'content': 'Rao, Atal Bihari Vajpayee, Manmohan Singh, and Narendra Modi, who is the current prime minister of India, serving since 26 May 2014. He is the first non-Congress leader to win three consecutive general elections and secure a third successive term (2014, 2019, 2024). The first prime minister to do so was Jawaharlal Nehru, who won the general elections of 1952, 1957, and 1962. [...] The prime minister of India (ISO: _Bhārata kē Pradhānamantrī_) is the head of government( of the Republic of India. Executive authority is vested in the prime minister and his chosen Council of Ministers,( despite the president of India being the nominal head of the executive.( The prime minister has to be a member of one of the houses of bicameral Parliament of India, alongside heading the respective house.( The prime minister and the cabinet are at all times responsible to the Lok Sabha.( [...] This article is part of a series on the\\nPolitics of India\\nImage 8: State Emblem of India\\nConstitution\\n\\n   Amendment\\n   Basic structure doctrine\\n   Fundamental Rights, Directive Principles and Fundamental Duties of India\\n   Human rights\\n   Judicial review\\n   Taxation\\n   Uniform Civil Code\\nGovernment\\n\\n   President of India\\n\\n   Droupadi Murmu (15th)\\n\\n  \\n\\n   Vice President of India\\n\\n   _vacant_ (15th)\\n\\n  \\n\\n   Prime Minister of India\\n\\n   Narendra Modi (14th)\\n\\n  \\n\\n   Cabinet secretary',\n",
       "  'score': 0.878825},\n",
       " {'title': 'Know the PM | Prime Minister of India',\n",
       "  'url': 'https://www.pmindia.gov.in/en/pms-profile/',\n",
       "  'content': 'Image 9: PM IndiaShri Narendra Modi was sworn-in as India’s Prime Minister for the third time on 9th June 2024, following another decisive victory in the 2024 Parliamentary elections. This victory marked the third consecutive term for Shri Modi, further solidifying his leadership. [...] The first ever Prime Minister to be born after Independence, Shri Modi has previously served as the Prime Minister of India from 2014 to 2019, and from 2019 to 2024. He also has the distinction of being the longest serving Chief Minister of Gujarat with his term spanning from October 2001 to May 2014.',\n",
       "  'score': 0.8687491}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily.invoke(\"Who is the PM of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74c39060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/anirudhsingh/learning_langchain/.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.14.1)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [bs4]\n",
      "\u001b[1A\u001b[2KSuccessfully installed beautifulsoup4-4.13.4 bs4-0.0.2 soupsieve-2.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "326cd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retriever\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=20)\n",
    "splitted_web_docs = WebBaseLoader(web_path=\"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\").load_and_split(text_splitter=splitter)\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "store = FAISS.from_documents(splitted_web_docs, embedding=embeddings)\n",
    "retiriever = store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8362a0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d6be10c8-ea91-4ea7-912d-eb1805d96b98', metadata={'source': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need', 'title': 'Attention Is All You Need - Wikipedia', 'language': 'en'}, page_content='Attention Is All You Need - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nAuthors\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nMethods discussed and introduced\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nHistorical context\\n\\n\\n\\n\\nToggle Historical context subsection\\n\\n\\n\\n\\n\\n3.1\\nPredecessors'),\n",
       " Document(id='8f350a5a-d440-4de4-b36d-fad348a99bd1', metadata={'source': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need', 'title': 'Attention Is All You Need - Wikipedia', 'language': 'en'}, page_content='Parallelizing attention[edit]\\nMain article: Attention (machine learning) §\\xa0History'),\n",
       " Document(id='80a500fd-d482-437e-88b0-23c47479321c', metadata={'source': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need', 'title': 'Attention Is All You Need - Wikipedia', 'language': 'en'}, page_content='\"Attention Is All You Need\"[1] is a 2017 landmark[2][3] research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al.[4] It is considered a foundational[5] paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models.[6][7] At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal generative'),\n",
       " Document(id='1009a6fd-eed3-49a0-8931-262ccc4ddfa7', metadata={'source': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need', 'title': 'Attention Is All You Need - Wikipedia', 'language': 'en'}, page_content='In the self-attention mechanism, queries (Q), keys (K), and values (V) are dynamically generated for each input sequence (limited typically by the size of the context window), allowing the model to focus on different parts of the input sequence at different steps. Multi-head attention enhances this process by introducing multiple parallel attention heads. Each attention head learns different linear projections of the Q, K, and V matrices. This allows the model to capture different aspects of the relationships between words in the sequence simultaneously, rather than focusing on a single aspect.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retiriever.invoke(\"What does attention actually mean?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23d6017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retiriever,\n",
    "    name='attention_retriever',\n",
    "    description='Helps find info about Attention is all you need research paper.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d1cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    tavily, retriever_tool,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f04fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa897ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0a20ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'meaning of life hindu culture'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'title': 'Hinduism: A Way Of Life Based On Eternal Truths - Harmony Circle', 'url': 'https://www.harmonycircle.sg/resources/religion/hinduism/', 'content': 'In Hinduism, the purpose of life is to recognise the essential oneness of existence. This, however, is often clouded by the illusion of duality â\\x80\\x93 the belief that one is separate from others and Brahman â\\x80\\x93 often reinforced by our experiences in the physical world. [...] Unlike other religions, Hinduism does not have a specific founder. Neither did it originate from a particular period of human history. Rather, it is a combination of philosophies, beliefs and practices that are recorded in ancient scriptures called the Vedas, meaning â\\x80\\x9cknowledgeâ\\x80\\x9d. [...] While Hinduism draws from many sources, the central focus of its practice is to free oneself from cycles of reincarnation by achieving enlightenment.\\n\\nThis is only possible when the soul is at one with Brahman, which is the absolute reality and the source of all things.', 'score': 0.70483154}, {'title': 'The purpose of life - Hindu beliefs - Edexcel', 'url': 'https://www.bbc.co.uk/bitesize/guides/zmvhsrd/revision/2', 'content': 'The purpose of life for Hindus is to achieve four aims, called PurusharthasclosePurusharthasA Sanskrit word used by Hindus meaning object of human truth.. These are dharma, kama, artha and moksha. These provide Hindus with opportunities to act morally and ethically and lead a good life. Throughout their lives, Hindus attempt to end the cycle of samsaraclosesamsaraIn Hinduism, Buddhism and Sikhism, this is the cycle of life, death and rebirth. and behave in a way that provides good [...] This Sanskrit word means love, desire and pleasure. It is a very practical part of Hindusâ\\x80\\x99 aims in life. Hindus aim to achieve pleasure in many ways, including sporting activities and cultural interests, but it is important to note that kama is also derived from sexual pleasure. Therefore, Hindus consider kama to be important in their personal lives. As Hinduism is a very practical religion, it is widely accepted that many Hindus attempt to achieve kama in their lives with their partners. [...] Moksha is the ultimate aim in life for Hindus. It means to be saved (salvation). When a Hindu achieves moksha, they break free from the cycle of samsara. Hindus aim to end the cycle of samsara through gaining good karma, which means doing good actions and deeds. Therefore, the actions of their previous lives and the actions of their mortal life today are all part of their effort to break the cycle of samsara, which each individual Hindu works towards.\\n\\n## More guides on this topic', 'score': 0.6768127}]\u001b[0m\u001b[32;1m\u001b[1;3mAccording to Hindu culture, the meaning of life is to recognize the essential oneness of existence and achieve enlightenment by freeing oneself from cycles of reincarnation. This can be achieved by following four aims in life: dharma (morality and ethics), kama (love, desire, and pleasure), artha (wealth and prosperity), and moksha (salvation). The ultimate goal is to break free from the cycle of samsara through gaining good karma by doing good actions and deeds.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the meaning of life as per hindu culture?',\n",
       " 'output': 'According to Hindu culture, the meaning of life is to recognize the essential oneness of existence and achieve enlightenment by freeing oneself from cycles of reincarnation. This can be achieved by following four aims in life: dharma (morality and ethics), kama (love, desire, and pleasure), artha (wealth and prosperity), and moksha (salvation). The ultimate goal is to break free from the cycle of samsara through gaining good karma by doing good actions and deeds.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.invoke({\"input\": \"What is the meaning of life as per hindu culture?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a7fd230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `attention_retriever` with `{'query': 'Attention is all you need paper'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mAttention Is All You Need - Wikipedia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jump to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\t\tNavigation\n",
      "\t\n",
      "\n",
      "\n",
      "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tContribute\n",
      "\t\n",
      "\n",
      "\n",
      "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Appearance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate\n",
      "\n",
      "Create account\n",
      "\n",
      "Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Personal tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate Create account Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tPages for logged out editors learn more\n",
      "\n",
      "\n",
      "\n",
      "ContributionsTalk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Top)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "Authors\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "Methods discussed and introduced\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "Historical context\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle Historical context subsection\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.1\n",
      "Predecessors\n",
      "\n",
      "\"Attention Is All You Need\"[1] is a 2017 landmark[2][3] research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al.[4] It is considered a foundational[5] paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models.[6][7] At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal generative\n",
      "\n",
      "Category\n",
      " Commons\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Retrieved from \"https://en.wikipedia.org/w/index.php?title=Attention_Is_All_You_Need&oldid=1303627119\"\n",
      "Categories: 2017 documentsArtificial intelligence papersGoogle2017 in artificial intelligenceHidden categories: Articles with short descriptionShort description is different from WikidataUse dmy dates from December 2023Articles containing potentially dated statements from 2025All articles containing potentially dated statements\n",
      "\n",
      "3.1\n",
      "Predecessors\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.2\n",
      "Attention with seq2seq\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.3\n",
      "Parallelizing attention\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.4\n",
      "AI boom era\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "Training\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "Notes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7\n",
      "External links\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle the table of contents\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "\n",
      "\n",
      "13 languages\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "العربيةCatalàEspañolفارسی한국어עבריתမြန်မာဘာသာ日本語PortuguêsРусскийTürkçeУкраїнська中文\n",
      "\n",
      "Edit links\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ArticleTalk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "English\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReadEditView history\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tools\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\t\tActions\n",
      "\t\n",
      "\n",
      "\n",
      "ReadEditView history\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tGeneral\n",
      "\t\n",
      "\n",
      "\n",
      "What links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tPrint/export\n",
      "\t\n",
      "\n",
      "\n",
      "Download as PDFPrintable version\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tIn other projects\n",
      "\t\n",
      "\n",
      "\n",
      "Wikidata item\u001b[0m\u001b[32;1m\u001b[1;3mThe \"Attention is all you need\" paper introduced a new deep learning architecture called the transformer, based on the attention mechanism. This architecture has become foundational in modern artificial intelligence and has contributed to the AI boom. The focus of the research was initially on improving Seq2seq techniques for machine translation, but it also showed potential for other tasks like question answering and multimodal generative models.\n",
      "\n",
      "Key aspects of the paper include:\n",
      "\n",
      "* Predecessors: The authors discussed the historical context and predecessors of their work.\n",
      "* Attention with seq2seq: They introduced the attention mechanism as a way to improve Seq2seq techniques.\n",
      "* Parallelizing attention: The authors showed how to parallelize the attention mechanism for faster computation.\n",
      "* AI boom era: The paper's introduction of the transformer architecture contributed to the AI boom and is considered a landmark in artificial intelligence research.\n",
      "\n",
      "Overall, the \"Attention is all you need\" paper has had a significant impact on the development of artificial intelligence and continues to influence research in this field.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Describe the attention is all you need paper.',\n",
       " 'output': 'The \"Attention is all you need\" paper introduced a new deep learning architecture called the transformer, based on the attention mechanism. This architecture has become foundational in modern artificial intelligence and has contributed to the AI boom. The focus of the research was initially on improving Seq2seq techniques for machine translation, but it also showed potential for other tasks like question answering and multimodal generative models.\\n\\nKey aspects of the paper include:\\n\\n* Predecessors: The authors discussed the historical context and predecessors of their work.\\n* Attention with seq2seq: They introduced the attention mechanism as a way to improve Seq2seq techniques.\\n* Parallelizing attention: The authors showed how to parallelize the attention mechanism for faster computation.\\n* AI boom era: The paper\\'s introduction of the transformer architecture contributed to the AI boom and is considered a landmark in artificial intelligence research.\\n\\nOverall, the \"Attention is all you need\" paper has had a significant impact on the development of artificial intelligence and continues to influence research in this field.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.invoke({\"input\": \"Describe the attention is all you need paper.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df6ca90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
